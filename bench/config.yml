default:
  recompute: FALSE # whether to recompute the efs, given that the result exists on the disk
  verbose: 0 # 0: don't print anything, 1: print some info, timings, etc.
  # which learner(s) to use in the benchmark (1 = use, 0 = don't use)
  use: !expr list(RSF = 1, AORSF = 1, XGBoost = 1, GLMBoost = 1, CoxBoost = 1, CoxLasso = 1)
  logging:
    bbotk_log: !expr lgr::get_logger("bbotk")$set_threshold("warn")
    mlr3_log: !expr lgr::get_logger("mlr3")$set_threshold("warn")
  workers: 40 # for parallelization
  efs:
    store_bmr: FALSE
    repeats: 100 # how many subsamples per learner
    ratio: 0.8 # % of training data for tuning/feature selection
  learner_params:
    n_trees: 500 # RSFs
    nrounds: 500 # xgboost, glmboost and cv_coxboost
    folds: 5 # for inner resampling/tuning
    evals: 25 # random search evaluations (glmboost)
    eta: 0.1 # learning rate (xgboost, glmboost)
    nrounds_early: 42 # early stopping rounds (xgboost, should be < nrounds)
    max_depth: 6 # xgboost
